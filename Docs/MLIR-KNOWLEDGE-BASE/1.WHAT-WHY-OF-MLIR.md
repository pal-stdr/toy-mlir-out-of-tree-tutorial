# 1. What is `MLIR` & Why we need that!!!ðŸ¤”ðŸ¤”ðŸ¤”

Million dollar question ðŸ˜…. They best way to understand what are them are to comparing them side by side.

Let's see an example input `C` code and output `MLIR` code. If you have an input `C` src code of Matrix Multiplication function:

```c
void matmul(DATA_TYPE A[N][K], DATA_TYPE B[K][M], DATA_TYPE C[N][M]) {
  int i, j, k;
  for (int i = 0; i < N; i++) {
    for (int j = 0; j < M; j++) {
      for (int k = 0; k < K; k++) {
        C[i][j] += A[i][k] * B[k][j];
      }
    }
  }
}
```

Then a possible MLIR code [snippet](https://polygeist.llvm.org/getting_started/Use_Polygeist/) could be

```llvm
func.func @matmul(
    %arg0: memref<?x400xf32>,
    %arg1: memref<?x300xf32>, 
    %arg2: memref<?x300xf32>
) attributes {llvm.linkage = #llvm.linkage<external>} {
    %c400 = arith.constant 400 : index
    %c300 = arith.constant 300 : index
    %c200 = arith.constant 200 : index
    %c0 = arith.constant 0 : index
    %c1 = arith.constant 1 : index
    scf.for %arg3 = %c0 to %c200 step %c1 {
      scf.for %arg4 = %c0 to %c300 step %c1 {
        scf.for %arg5 = %c0 to %c400 step %c1 {
          %0 = memref.load %arg0[%arg3, %arg5] : memref<?x400xf32>
          %1 = memref.load %arg1[%arg5, %arg4] : memref<?x300xf32>
          %2 = arith.mulf %0, %1 : f32
          %3 = memref.load %arg2[%arg3, %arg4] : memref<?x300xf32>
          %4 = arith.addf %3, %2 : f32
          memref.store %4, %arg2[%arg3, %arg4] : memref<?x300xf32>
        }
      }
    }
    return
  }
```
which is actually the visible and usable form of an MLIR Dialect for this particular Matrix Multiplication input `C` code.

`MLIR` stands for `Muti-Level Intermediate Representation`. It's the official name of the Compiler Infrastructure as like as `LLVM`. And `Dialect` is the form of that `IR` (i.e. `MLIR`). So MLIR is a compiler infrastructure that aims to provide a unified and extensible framework for representing and manipulating code across various domains; such as, machine learning frameworks (e.g. `Tensorflow`, `PyTorch`, etc.), high-level languages (e.g. `C`, `C++`, `Python`, `Swift`, etc.), and hardware accelerators (e.g. `GPU`, `CPU`, `FPGA`, `TPU`, etc.).

In simpler terms, **MLIR is like a toolbox for representing and manipulating code, while dialects are like the different tools in that toolbox. Each dialect is designed to handle a specific type of code or computation, and they can be combined together to represent more complex scenarios.**


# 2. Why `MLIR`?

## 2.1. One `IR` to represent all kind of language specific transformation

Official [Toy Tutorial](https://mlir.llvm.org/docs/Tutorials/Toy/Ch-2/) 2nd chapter's section [Introduction: Multi-Level Intermediate Representation](https://mlir.llvm.org/docs/Tutorials/Toy/Ch-2/#introduction-multi-level-intermediate-representation
) says

> Other compilers, like LLVM (see the Kaleidoscope tutorial), offer a fixed set of predefined types and (usually low-level / RISC-like) instructions. It is up to the frontend for a given language to perform any language-specific type-checking, analysis, or transformation before emitting LLVM IR. For example, `Clang` will use its AST to perform not only static analysis but also transformations, such as C++ template instantiation through AST cloning and rewrite. Finally, languages with construction at a higher-level than C/C++ may require non-trivial lowering from their AST to generate LLVM IR. As a consequence, multiple frontends end up reimplementing significant pieces of infrastructure to support the need for these analyses and transformation. MLIR addresses this issue by being designed for extensibility. As such, there are few pre-defined instructions (operations in MLIR terminology) or types.

Daunting as a Newbie to the compiler world ðŸ˜!! Let's try to understand what all those things are about...

Let's try to see the compilation process flow in a very very very abstract way. And then we will try to understand the paragraph given above in the context of following compilation flow.

```sh
# Typical compilation process in bird's eye view
# Here, The front end part is upto IR gen. So the output of the frontend is IR.
# So from the IR, the lowering starts.
# Static analyses and transformations can be done anywhere (i.e. Not only in frontend part, but also while lowering the IR); upto the final binary.
Src code --> Lexer + Parser + AST (Abstract Syntax Tree) gen --> IR --> Binary
```

`Clang` uses it's AST to perform static analysis and transformation on advanced `C++` language features (e.g. `Template instantiation`) for lowering. Similarly, other high level languages like `Python`, `Swift`, etc. has their own infrastructure to handle analyses, transformations on their own language specific features, and then preparing for final lowering.

So the issue here is, all the languages are busy with engineering the complicated infrastructure to support their own powerful features. However, the sad part is, one language cannot share their advanced, efficient & sophisticated infrastructure with other language. Moreover, If anybody want to create a new language by combining the best features of existing languages; would be tedious, time consuming and costly.ðŸ¥´

Now you can say, we can do that by using `LLVM`. Okay, partially true. But `LLVM` has a fixed set of `instructions` and `types` that they can use to represent code. So this can make it difficult to use LLVM to represent code from different languages or with different features. To put it simply, LLVM is like a general-purpose toolset, and it can be challenging to use for specialized tasks.

On the other hand, Here comes the mighty `MLIR`. It is more like a modular toolkit, making it easier to customize and adapt to specific needs.

**Example from the context of `type` in `MLIR` vs `LLVM`**

n MLIR, it is possible to define new types and type operators. This allows MLIR to be adapted to different use cases, such as representing machine learning models or hardware accelerators. E.g. You can represent `multidimensional array` with `tensor`, `vector` & `memref` in MLIR; such as `tensor<100x500xf32>`, `vector<3x5x6x9xf16>` `memref<2x6xi32>`. `tensor` can be dynamically `shaped`, `unranked`, or have `0 dimensions`. You can have a `memref` (a buffer in memory) containing `vectors`. Lot lot more...

In LLVM, it is not possible to define new types or type operators. This makes it less flexible, as it is not possible to represent all possible types.


## 2.2. Answering the `Why`, from what it can do!!

### 2.2.1 Machine learning:

MLIR can be used to represent and manipulate machine learning models, including neural networks, natural language processing models, and reinforcement learning models, etc.

### 2.2.2. High-level languages:

MLIR can be used to represent code written in high-level languages, such as `Python`, `C++`, `Java`, etc. This allows MLIR to be used to integrate machine learning models into existing codebases.

### 2.2.3 Hardware accelerators:

MLIR can be used to represent code that is executed on hardware accelerators, such as `GPUs`, `FPGAs`, and specialized hardware accelerators. This allows MLIR to be used to optimize machine learning models for specific hardware platforms.


### 2.2.4. Domain-specific languages:

MLIR can be used to represent Domain-Specific Languages (DSLs), which are languages that are tailored to specific domains, such as image processing or control systems.

### 2.2.5. Combining all of them:

Consider a machine learning model that needs to be trained and deployed on a hardware accelerator. The MLIR framework can be used to represent the model in an abstract way, and then it can be converted into the specific dialect of the hardware accelerator (i.e. `CPU`, `GPU`, `FPGA`, etc.). This allows the model to be trained on a general-purpose machine and then deployed on the hardware accelerator without any additional translation.

MLIR and dialects are powerful tools that can be used to represent and manipulate code more efficiently and effectively. They are particularly useful for machine learning and hardware acceleration, where it is important to be able to represent complex computations in a way that is both flexible and performant.